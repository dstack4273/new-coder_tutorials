<!DOCTYPE html>

<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="author" content="Lynn Root">
<meta name="description" content="Five Life Jackets to Throw at the New Coder - Python tutorials">
<meta name="generator" content="mynt v0.2.2">

<link rel="shortcut icon" href="/assets/images/favicon.ico" type="image/x-icon">



<link rel="stylesheet" href="/assets/css/screen.css" type="text/css">
<link rel="stylesheet" href="/assets/css/style.css" type="text/css">
<link rel="stylesheet" href="/assets/css/glyphicons.css" type="text/css">
<link rel="stylesheet" href="/assets/css/pygments.css" type="text/css">
<script src="/assets/js/modernizr.js"></script>


    
    <title>Part 4: Pipelining Data to Database &ndash; New Coder</title>
</head>

<body>
    <div class="ribbon">
      <a href="https://github.com/econchick/new-coder">Contribute on GitHub</a>
    </div>
    <div id="container">
        <div id="nav">
            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="/about/">About</a></li>
                <li><a href="/tutorials/">Tutorials</a></li>
                <li><a href="/contact/">Need help? Want to say hi?</a></li>
            </ul>
            
        </div>
        
        <div id="header">
            <h1><a href="/">New C<span aria-hidden="true" class="icon" data-icon="&#xe308;">der</a></h1>
            <h2>Five Life Jackets to Throw at the New Coder</h2>
        </div>

        <div id="content">
            
    <div class="item">
        <div class="header">
            <h2>Part 4: Pipelining Data to Database</h2>
        </div>
        
        <div class="body">
            <p>Pipelining our scraped data into our Postgres database.</p>

<p><strong>TODO</strong> add commit/rollback intro</p>
<h3>Setup our Pipeline</h3>
<p>We&#39;ve setup our spider to crawl and parse the HTML, and we&#39;ve set up our database to take the parse data. Now we have to connect the two together through a pipeline.</p>

<p>In <code>pipelines.py</code>, we will define a session (the actual connecting to the database), as well as the feeding/writing of data to the database.</p>

<p>We&#39;ll need to import SQLAlchemy&#39;s <code>sessionmaker</code> function to bind to the database (create that connection), as well as import our models.</p>
<div class="code"><div><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25</pre></div></td><td class="code"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sqlalchemy.orm</span> <span class="kn">import</span> <span class="n">sessionmaker</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">Deals</span><span class="p">,</span> <span class="n">db_connect</span><span class="p">,</span> <span class="n">create_deals_table</span>


<span class="k">class</span> <span class="nc">LivingSocialPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Livingsocial pipeline for storing scraped items in the database&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes database connection and sessionmaker.</span>
<span class="sd">        Creates deals table.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">db_connect</span><span class="p">()</span>
        <span class="n">create_deals_table</span><span class="p">(</span><span class="n">engine</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Session</span> <span class="o">=</span> <span class="n">sessionmaker</span><span class="p">(</span><span class="n">bind</span><span class="o">=</span><span class="n">engine</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save deals in the database.</span>
<span class="sd">        This method is called for every item pipeline component.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">session</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
        <span class="n">deal</span> <span class="o">=</span> <span class="n">Deals</span><span class="p">(</span><span class="o">**</span><span class="n">item</span><span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">deal</span><span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</td></tr></table></div></div>
<p>Here, we create a class, <code>LivingSocialPipeline()</code>.  We have a constructor function, <code>def __init__(self)</code> to initialize the class by defining the engine, the deals table, and binding/connecting to the database with the defined engine.</p>

<p>We then define <code>save_item()</code> that takes the parameters, <code>item</code> and <code>spider</code>. We establish a session with the database, then unpack an item (the data of one scraped deal) within our <code>Deal()</code> model.  We then add the <code>deal</code> to our database by calling <code>session.add()</code> – this is the actual writing to the database with our information.</p>

<p>Finally, we commit the the transaction (a transaction here would be the act of writing to the database). The reason why we have to commit after adding data is </p>

<p><strong>TODO</strong> Do we need to <code>return item</code>?</p>
<h3>Return to settings.py</h3>
<p>Nearly there – we need to add a variable to <code>settings.py</code> that tells scrapy where to find our pipeline when processing data.</p>

<p>So within <code>settings.py</code>, add another variable, <code>ITEM_PIPELINES</code>:</p>
<div class="code"><div><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;living_social.pipelines.LivingSocialPipeline&#39;</span><span class="p">]</span>
</pre></div>
</td></tr></table></div></div>
<p>This is the directory/module path to the pipeline we just defined.</p>

<p><a href="/Part-5-Running-our-scraper">Part 5 puts the project all together &rarr;</a></p>

        </div>
    </div>

        </div>
        
        <div id="footer">
            <p>Copyright &copy; 2013 Lynn Root &ndash; powered by <a href="http://mynt.mirroredwhite.com/">mynt</a></p>
        </div>
    </div>
</body>
</html>